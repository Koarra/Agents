{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41195ea-5326-481c-8880-e60224906724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "G-Eval Result:\n",
      " {\n",
      "  \"factual_accuracy\": 4,\n",
      "  \"completeness\": 2,\n",
      "  \"helpfulness\": 3,\n",
      "  \"justification\": \"The candidate answer is factually accurate in that John Smith was indeed mentioned as a main suspect, but it adds an incorrect detail about the verdict. This reduces its completeness score to 2. However, it still provides some useful information, making it somewhat helpful.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "\n",
    "# Initialize Ollama client\n",
    "client = Client()\n",
    "\n",
    "# Define inputs\n",
    "question = \"Who committed the crime in the document?\"\n",
    "candidate_answer = \"John Smith was found guilty of financial fraud in 2020.\"\n",
    "reference_answer = \"The document states that John Smith was the main suspect, but no verdict was mentioned.\"\n",
    "\n",
    "# Build G-Eval style prompt\n",
    "prompt = f\"\"\"\n",
    "You are an evaluator judging the quality of a generated answer.\n",
    "\n",
    "Task: Evaluate the candidate answer to the question based on the reference answer.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Candidate Answer:\n",
    "{candidate_answer}\n",
    "\n",
    "Reference Answer:\n",
    "{reference_answer}\n",
    "\n",
    "Evaluate based on:\n",
    "- Factual Accuracy (Does it reflect the reference accurately?)\n",
    "- Completeness (Does it include all important details?)\n",
    "- Helpfulness (Would this answer help a human understand?)\n",
    "\n",
    "Please respond in this exact JSON format:\n",
    "{{\n",
    "  \"factual_accuracy\": 1-5,\n",
    "  \"completeness\": 1-5,\n",
    "  \"helpfulness\": 1-5,\n",
    "  \"justification\": \"Short explanation here\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# Call the local model via Ollama\n",
    "response = client.chat(\n",
    "    model='llama3.1',\n",
    "    messages=[{'role': 'user', 'content': prompt}]\n",
    ")\n",
    "\n",
    "# Output the result\n",
    "print(\"\\nG-Eval Result:\\n\", response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88296ef-a70d-45d7-8c8d-69aa109d0c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casstle",
   "language": "python",
   "name": "casstle_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
