================================================================================
SIAP COMPLIANCE DETECTION SYSTEM - Technical Explanation
================================================================================

1. SOLUTION OVERVIEW
================================================================================

This system implements an autonomous compliance detection pipeline using
LangGraph and the ReAct (Reasoning + Acting) pattern with Azure OpenAI.

Key Innovation: Instead of relying solely on LLM reasoning, we combine:
- LLM intelligence for understanding context and nuance
- Deterministic tools for grounding (evidence extraction, threshold validation)
- Decision tree traversal with skip logic for efficient processing

Architecture:
    Document -> Router -> Question Loop (ReAct Agent) -> Verdict
                 |              |
                 v              v
           Scenario        Tools: search_evidence()
           Classification        check_threshold()


1b. WHY KEYWORD FUNCTIONS? (The Core Design Decision)
================================================================================

THE PROBLEM WE'RE SOLVING:

If you ask an LLM "Does this document mention cannabis revenue over 10%?",
it might respond: "Yes, the document states 45% of revenue comes from cannabis."

But what if the document actually says 25%? Or doesn't mention a percentage at all?
LLMs can HALLUCINATE - they confidently state things that aren't in the text.

In compliance work, this is dangerous. A false positive could block a legitimate
client. A false negative could expose the bank to regulatory fines.


WHY KEYWORD/REGEX TOOLS EXIST:

The keyword functions (extract_text_evidence, classify_scenario) serve as
GROUNDING MECHANISMS. They force the system to:

1. PROVE IT: Instead of the LLM saying "I found evidence", the tool returns
   the ACTUAL text snippet: "...dispensary operations generate 60% of revenue..."

2. NO INVENTION: The tool can only return text that EXISTS in the document.
   It cannot hallucinate quotes.

3. AUDIT TRAIL: Compliance officers can verify: "Show me where it says that."
   The system can point to exact passages.

4. DETERMINISTIC: Same input = same output. No randomness from LLM temperature.


ANALOGY:

Think of it like a courtroom:
- The LLM is the lawyer making arguments
- The keyword tool is the evidence clerk who retrieves actual documents
- The lawyer can interpret, but cannot fabricate evidence


ALTERNATIVES TO KEYWORD FUNCTIONS:
================================================================================

YES, there are alternatives. Here's the spectrum from simple to sophisticated:

OPTION 1: PURE LLM (No Tools)
-----------------------------
Just ask: "Read this document and answer: Is the client in cannabis business?"

Pros:
  + Simplest implementation
  + Understands context, synonyms, implications
  + Works in any language

Cons:
  - Can hallucinate evidence
  - No proof of where information came from
  - Hard to audit/verify
  - Inconsistent results (temperature affects output)

When to use: Low-stakes applications, initial prototyping


OPTION 2: KEYWORD/REGEX SEARCH (Current Implementation)
-------------------------------------------------------
Use regex patterns to find and extract matching text.

Pros:
  + Fast and deterministic
  + Returns actual document text
  + Easy to implement and debug
  + No additional infrastructure

Cons:
  - Language dependent (English stop words)
  - Misses synonyms ("marijuana" vs "weed" vs "pot")
  - Can't understand context
  - Exact match only

When to use: English documents, known terminology, need for speed


OPTION 3: SEMANTIC/EMBEDDING SEARCH (Recommended Upgrade)
---------------------------------------------------------
Convert text to vectors, find semantically similar passages.

Implementation:
  from langchain_openai import AzureOpenAIEmbeddings

  embeddings = AzureOpenAIEmbeddings(model="text-embedding-3-small")
  query_vector = embeddings.embed_query("cannabis revenue percentage")
  # Find similar chunks in document using cosine similarity

Pros:
  + Language agnostic (embeddings work across languages)
  + Understands synonyms and related concepts
  + "cannabis revenue" matches "marijuana sales income"
  + Better recall (finds relevant passages keyword search misses)

Cons:
  - Requires embedding model calls (added latency/cost)
  - Need to chunk documents appropriately
  - Similarity threshold tuning required
  - More complex implementation

When to use: Multilingual, varied terminology, higher accuracy needs


OPTION 4: RAG WITH VECTOR DATABASE
----------------------------------
Pre-index documents in a vector store (FAISS, Pinecone, Azure AI Search).

Pros:
  + Scales to millions of documents
  + Fast retrieval after indexing
  + Combines semantic search with metadata filtering

Cons:
  - Infrastructure overhead
  - Index maintenance
  - Overkill for single-document analysis

When to use: Large document repositories, production systems


OPTION 5: HYBRID APPROACH (Best of Both Worlds)
-----------------------------------------------
Combine keyword AND semantic search:

  def search_evidence_hybrid(query, document):
      # First: Fast keyword scan for exact matches
      keyword_results = extract_text_evidence(query, document)

      # Second: Semantic search for related concepts
      semantic_results = semantic_search(query, document)

      # Merge and deduplicate
      return merge_results(keyword_results, semantic_results)

Pros:
  + Catches exact matches (keyword) AND related concepts (semantic)
  + Redundancy improves recall
  + Can weight results by match type

Cons:
  - Most complex to implement
  - Highest latency (two search methods)

When to use: Critical compliance decisions, maximum accuracy needed


RECOMMENDATION:
---------------
For your use case (compliance detection), I recommend upgrading to OPTION 3
(Semantic Search) or OPTION 5 (Hybrid). Here's why:

1. Compliance documents use varied terminology
2. Multilingual support is likely needed
3. Missing a risk indicator has serious consequences
4. The added cost of embeddings is justified by accuracy gains

The current keyword implementation is a GOOD STARTING POINT - it's fast,
debuggable, and proves the architecture works. Semantic search is the
natural evolution when you need better coverage.


2. WHY THIS APPROACH? (Selling Points)
================================================================================

A) GROUNDED REASONING
   - LLMs can hallucinate, especially with numbers and specific facts
   - Our tools extract ACTUAL quotes from the document
   - Threshold checks use Python math, not LLM arithmetic
   - Evidence is traceable back to source text

B) AUDITABLE DECISIONS
   - Every YES/NO decision includes:
     * The exact question asked
     * Evidence snippets from the document
     * Confidence score
     * Reasoning chain
   - Compliance officers can review and verify

C) CONFIGURABLE SCENARIOS
   - Decision trees defined in JSON files
   - New compliance scenarios added without code changes
   - Skip logic (if Q1=NO, skip to Q3) reduces unnecessary processing

D) COST EFFICIENT
   - ReAct pattern minimizes token usage
   - Tools do the heavy lifting (regex search vs LLM scanning)
   - Batch processing with parallel execution

E) AZURE AD AUTHENTICATION
   - Enterprise-grade security (no API keys in code)
   - Uses DefaultAzureCredential (works with managed identity, CLI, etc.)
   - Compliant with corporate security policies


3. PROS AND CONS
================================================================================

PROS:
-----
+ Explainable AI: Every decision has evidence trail
+ Reduced hallucination: Tools ground the LLM's reasoning
+ Flexible: JSON-based scenario configuration
+ Scalable: Batch processing with thread pool
+ Secure: Azure AD token authentication
+ Maintainable: Clean separation (state, tools, nodes, graph)
+ Testable: Each component can be unit tested

CONS:
-----
- Latency: Multiple LLM calls per document (router + questions)
- Token cost: ReAct loop may require several iterations
- Keyword dependency: Initial classification relies on keyword matching
- English-centric: Stop words and keywords are English-only (see below)
- Threshold rigidity: 75% confidence threshold may need tuning per use case
- Tool limitations: Only 2 tools (search_evidence, check_threshold)


4. MULTILINGUAL DOCUMENTS - IMPORTANT CONSIDERATION
================================================================================

QUESTION: What if documents are in another language (French, German, etc.)?

CURRENT LIMITATIONS:

A) extract_text_evidence() in tools.py (lines 33-35):

   The stop_words set is English-only:
   stop_words = {"is", "the", "a", "an", "does", "are", "there", ...}

   IMPACT: For French documents, words like "le", "la", "les", "un", "une"
   would NOT be filtered, polluting search results.

B) classify_scenario() in tools.py (lines 196-200):

   Keyword patterns are English:
   keyword_patterns = {
       "cannabis": ["cannabis", "marijuana", "hemp", "thc", ...],
       "art": ["art", "antique", "auction", "gallery", ...],
   }

   IMPACT: A French document about "galerie d'art" or "antiquités"
   would NOT match the English keywords.

C) ReAct prompts in nodes.py (lines 132-148):

   System prompt is in English:
   "You are a compliance analyst. Your task is to answer..."

   IMPACT: Works fine - GPT-4 understands English instructions even when
   analyzing non-English documents. BUT responses may mix languages.


RECOMMENDED SOLUTIONS FOR MULTILINGUAL SUPPORT:

Option 1: LANGUAGE-SPECIFIC STOP WORDS
   Add language detection and use appropriate stop word lists:

   from langdetect import detect
   STOP_WORDS = {
       "en": {"is", "the", "a", ...},
       "fr": {"le", "la", "les", "un", "une", "de", "du", ...},
       "de": {"der", "die", "das", "ein", "eine", ...},
   }

Option 2: MULTILINGUAL KEYWORDS
   Expand keyword patterns to include translations:

   keyword_patterns = {
       "cannabis": ["cannabis", "marijuana", "chanvre", "Hanf", ...],
       "art": ["art", "kunst", "galerie", "gallery", "antiquité", ...],
   }

Option 3: LLM-BASED CLASSIFICATION (Already Implemented as Fallback)
   The router_node already falls back to LLM classification when keywords
   don't match (nodes.py lines 62-98). This handles any language naturally.

   RECOMMENDATION: Make LLM classification the primary method for
   multilingual deployments.

Option 4: TRANSLATE BEFORE PROCESSING
   Use Azure Translator to convert documents to English first.
   PRO: All existing tools work unchanged
   CON: Added latency and cost, potential translation errors

Option 5: SEMANTIC SEARCH INSTEAD OF KEYWORD SEARCH
   Replace extract_text_evidence() with embedding-based search:
   - Embed the query using Azure OpenAI embeddings
   - Embed document chunks
   - Find semantically similar chunks
   PRO: Language-agnostic
   CON: Requires vector database, more complex setup


WHAT WORKS WITHOUT CHANGES:

+ LLM understanding: GPT-4/GPT-4o can read and understand any language
+ Question answering: The ReAct agent can reason about French/German docs
+ check_threshold(): Pure math - language independent
+ Decision tree logic: Node traversal is language independent

WHAT NEEDS ADAPTATION:

- Stop words in extract_text_evidence()
- Keywords in classify_scenario()
- Optionally: Prompts (for response language consistency)


5. TECHNICAL DETAILS
================================================================================

Authentication Flow:
   DefaultAzureCredential tries (in order):
   1. Environment variables (AZURE_CLIENT_ID, etc.)
   2. Managed Identity (Azure VMs, App Service)
   3. Azure CLI (az login)
   4. Visual Studio / VS Code credentials

   Token is obtained for scope: https://cognitiveservices.azure.com/.default

ReAct Agent Flow:
   1. Agent receives question + article context
   2. THINK: What evidence do I need?
   3. ACT: Call search_evidence("cannabis income")
   4. OBSERVE: "EVIDENCE FOUND: ...60% of revenue from dispensary..."
   5. THINK: Need to verify the percentage threshold
   6. ACT: Call check_threshold(60.0, 10.0)
   7. OBSERVE: "EXCEEDS THRESHOLD: 60% > 10%"
   8. CONCLUDE: "FINAL ANSWER: YES"

State Management:
   ComplianceState (TypedDict) tracks:
   - article_id, article_text
   - scenario_id, scenario_name
   - current_node (Q1, Q2, etc.)
   - decision_tree (loaded from JSON)
   - answers (dict of AnswerRecord)
   - final_verdict (HIT/NO_HIT/PENDING)
   - risk_score (0.0 to 1.0)


6. CONFIGURATION
================================================================================

Environment Variables:
   AZURE_OPENAI_ENDPOINT    - Required (e.g., https://your-resource.openai.azure.com/)
   AZURE_OPENAI_DEPLOYMENT  - Optional (default: gpt-4o-mini)
   AZURE_OPENAI_API_VERSION - Optional (default: 2024-02-01)

Scenario JSON Format (in scenarios_1/):
   {
     "name": "Cannabis Business",
     "description": "...",
     "start": "Q1",
     "questions": {
       "Q1": {
         "text": "Is the client involved in cannabis?",
         "next_if_yes": "Q2",
         "next_if_no": null
       },
       "Q2": {...}
     }
   }


7. FUTURE ENHANCEMENTS
================================================================================

- Add more tools: web search, database lookup, document comparison
- Implement caching for repeated document analysis
- Add confidence calibration based on historical accuracy
- Support document chunking for very long documents
- Add multilingual stop word lists
- Implement semantic search with embeddings
- Add human-in-the-loop review for low-confidence decisions

================================================================================
